{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97432eed",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5f9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6c742",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc80d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367df7ab",
   "metadata": {},
   "source": [
    "Objects in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9217aa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'person',\n",
       " 1: 'bicycle',\n",
       " 2: 'car',\n",
       " 3: 'motorcycle',\n",
       " 4: 'airplane',\n",
       " 5: 'bus',\n",
       " 6: 'train',\n",
       " 7: 'truck',\n",
       " 8: 'boat',\n",
       " 9: 'traffic light',\n",
       " 10: 'fire hydrant',\n",
       " 11: 'stop sign',\n",
       " 12: 'parking meter',\n",
       " 13: 'bench',\n",
       " 14: 'bird',\n",
       " 15: 'cat',\n",
       " 16: 'dog',\n",
       " 17: 'horse',\n",
       " 18: 'sheep',\n",
       " 19: 'cow',\n",
       " 20: 'elephant',\n",
       " 21: 'bear',\n",
       " 22: 'zebra',\n",
       " 23: 'giraffe',\n",
       " 24: 'backpack',\n",
       " 25: 'umbrella',\n",
       " 26: 'handbag',\n",
       " 27: 'tie',\n",
       " 28: 'suitcase',\n",
       " 29: 'frisbee',\n",
       " 30: 'skis',\n",
       " 31: 'snowboard',\n",
       " 32: 'sports ball',\n",
       " 33: 'kite',\n",
       " 34: 'baseball bat',\n",
       " 35: 'baseball glove',\n",
       " 36: 'skateboard',\n",
       " 37: 'surfboard',\n",
       " 38: 'tennis racket',\n",
       " 39: 'bottle',\n",
       " 40: 'wine glass',\n",
       " 41: 'cup',\n",
       " 42: 'fork',\n",
       " 43: 'knife',\n",
       " 44: 'spoon',\n",
       " 45: 'bowl',\n",
       " 46: 'banana',\n",
       " 47: 'apple',\n",
       " 48: 'sandwich',\n",
       " 49: 'orange',\n",
       " 50: 'broccoli',\n",
       " 51: 'carrot',\n",
       " 52: 'hot dog',\n",
       " 53: 'pizza',\n",
       " 54: 'donut',\n",
       " 55: 'cake',\n",
       " 56: 'chair',\n",
       " 57: 'couch',\n",
       " 58: 'potted plant',\n",
       " 59: 'bed',\n",
       " 60: 'dining table',\n",
       " 61: 'toilet',\n",
       " 62: 'tv',\n",
       " 63: 'laptop',\n",
       " 64: 'mouse',\n",
       " 65: 'remote',\n",
       " 66: 'keyboard',\n",
       " 67: 'cell phone',\n",
       " 68: 'microwave',\n",
       " 69: 'oven',\n",
       " 70: 'toaster',\n",
       " 71: 'sink',\n",
       " 72: 'refrigerator',\n",
       " 73: 'book',\n",
       " 74: 'clock',\n",
       " 75: 'vase',\n",
       " 76: 'scissors',\n",
       " 77: 'teddy bear',\n",
       " 78: 'hair drier',\n",
       " 79: 'toothbrush'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928bdd8",
   "metadata": {},
   "source": [
    "Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9592b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('input.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3326006",
   "metadata": {},
   "source": [
    "Detect objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f9da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 4 persons, 1 dog, 1 frisbee, 146.7ms\n",
      "Speed: 13.4ms preprocess, 146.7ms inference, 15.1ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    }
   ],
   "source": [
    "result = model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004ce90",
   "metadata": {},
   "source": [
    "Show the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f9b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009828f",
   "metadata": {},
   "source": [
    "Loop through all boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839981fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 0.,  0.,  0.,  0., 16., 29.])\n",
      "conf: tensor([0.9250, 0.9238, 0.8919, 0.8824, 0.8775, 0.2773])\n",
      "data: tensor([[4.2191e+02, 9.9434e+01, 6.5219e+02, 4.9623e+02, 9.2496e-01, 0.0000e+00],\n",
      "        [8.1112e+02, 1.2329e+02, 1.0253e+03, 5.0358e+02, 9.2379e-01, 0.0000e+00],\n",
      "        [1.3040e+02, 1.5664e+02, 2.6757e+02, 5.0614e+02, 8.9190e-01, 0.0000e+00],\n",
      "        [2.9722e+02, 1.9584e+02, 4.2175e+02, 4.9498e+02, 8.8239e-01, 0.0000e+00],\n",
      "        [6.2975e+02, 3.4144e+02, 7.4409e+02, 5.0165e+02, 8.7751e-01, 1.6000e+01],\n",
      "        [2.3970e+02, 2.1747e+02, 2.6665e+02, 2.8915e+02, 2.7726e-01, 2.9000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (597, 1100)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[537.0510, 297.8339, 230.2728, 396.7997],\n",
      "        [918.2189, 313.4348, 214.1977, 380.2874],\n",
      "        [198.9844, 331.3923, 137.1730, 349.4967],\n",
      "        [359.4871, 345.4082, 124.5337, 299.1371],\n",
      "        [686.9200, 421.5468, 114.3357, 160.2126],\n",
      "        [253.1752, 253.3080,  26.9503,  71.6847]])\n",
      "xywhn: tensor([[0.4882, 0.4989, 0.2093, 0.6647],\n",
      "        [0.8347, 0.5250, 0.1947, 0.6370],\n",
      "        [0.1809, 0.5551, 0.1247, 0.5854],\n",
      "        [0.3268, 0.5786, 0.1132, 0.5011],\n",
      "        [0.6245, 0.7061, 0.1039, 0.2684],\n",
      "        [0.2302, 0.4243, 0.0245, 0.1201]])\n",
      "xyxy: tensor([[ 421.9146,   99.4341,  652.1874,  496.2337],\n",
      "        [ 811.1201,  123.2912, 1025.3177,  503.5786],\n",
      "        [ 130.3979,  156.6439,  267.5710,  506.1406],\n",
      "        [ 297.2202,  195.8396,  421.7539,  494.9767],\n",
      "        [ 629.7521,  341.4404,  744.0878,  501.6531],\n",
      "        [ 239.7000,  217.4657,  266.6503,  289.1504]])\n",
      "xyxyn: tensor([[0.3836, 0.1666, 0.5929, 0.8312],\n",
      "        [0.7374, 0.2065, 0.9321, 0.8435],\n",
      "        [0.1185, 0.2624, 0.2432, 0.8478],\n",
      "        [0.2702, 0.3280, 0.3834, 0.8291],\n",
      "        [0.5725, 0.5719, 0.6764, 0.8403],\n",
      "        [0.2179, 0.3643, 0.2424, 0.4843]])\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(r.boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3f2a4",
   "metadata": {},
   "source": [
    "Check the class and confidence of boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5470ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.924964427947998\n",
      "tensor(421.9146) tensor(99.4341) tensor(652.1874) tensor(496.2337)\n",
      "********************\n",
      "0\n",
      "0.9237902164459229\n",
      "tensor(811.1201) tensor(123.2912) tensor(1025.3177) tensor(503.5786)\n",
      "********************\n",
      "0\n",
      "0.8919014930725098\n",
      "tensor(130.3979) tensor(156.6439) tensor(267.5710) tensor(506.1406)\n",
      "********************\n",
      "0\n",
      "0.8823862671852112\n",
      "tensor(297.2202) tensor(195.8396) tensor(421.7539) tensor(494.9767)\n",
      "********************\n",
      "16\n",
      "0.8775050640106201\n",
      "tensor(629.7521) tensor(341.4404) tensor(744.0878) tensor(501.6531)\n",
      "********************\n",
      "29\n",
      "0.27725714445114136\n",
      "tensor(239.7000) tensor(217.4657) tensor(266.6503) tensor(289.1504)\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    boxes = r.boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        x1, y1, x2, y2 = box.xyxy[0]\n",
    "\n",
    "        print(cls_id)\n",
    "        print(conf)\n",
    "        print(x1, y1, x2, y2)\n",
    "\n",
    "        print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78194d91",
   "metadata": {},
   "source": [
    "Create directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26cc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'slices'\n",
    "os.makedirs(save_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2283f60",
   "metadata": {},
   "source": [
    "Crop and save the images of humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4721bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = 0\n",
    "for r in result:\n",
    "    boxes = r.boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "\n",
    "        if cls_id == 0 and conf > .5:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "            crop_img = img[y1:y2 , x1:x2]\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"person_{persons}.jpg\")\n",
    "            cv2.imwrite(save_path, crop_img)\n",
    "\n",
    "            persons += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
